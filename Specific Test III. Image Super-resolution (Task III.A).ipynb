{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "338ff175-d6b7-4b31-aab1-a54493765117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Necessary Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import log10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc4a8fc9-2f49-47f6-807a-ed0423952407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrongLensingDataset(Dataset):\n",
    "    def __init__(self, lr_files, hr_files):\n",
    "        self.lr_files = lr_files\n",
    "        self.hr_files = hr_files\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lr_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lr_path = self.lr_files[idx]\n",
    "        hr_path = self.hr_files[idx]\n",
    "        lr_img = np.load(lr_path)  # expected shape: (H, W) or (C, H, W)\n",
    "        hr_img = np.load(hr_path)\n",
    "        \n",
    "        # If the images are single-channel (H, W), add a channel dimension.\n",
    "        if len(lr_img.shape) == 2:\n",
    "            lr_img = np.expand_dims(lr_img, axis=0)\n",
    "        if len(hr_img.shape) == 2:\n",
    "            hr_img = np.expand_dims(hr_img, axis=0)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        lr_img = lr_img.astype(np.float32) / (lr_img.max() if lr_img.max() != 0 else 1)\n",
    "        hr_img = hr_img.astype(np.float32) / (hr_img.max() if hr_img.max() != 0 else 1)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        lr_tensor = torch.from_numpy(lr_img)\n",
    "        hr_tensor = torch.from_numpy(hr_img)\n",
    "        \n",
    "        return lr_tensor, hr_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e732e3d7-968d-44ea-9648-f710ffd74824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these paths to point to your local dataset directories.\n",
    "lr_folder = \"/Users/EndUser/Downloads/dataset-2/LR\"\n",
    "hr_folder = \"/Users/EndUser/Downloads/dataset-2/HR\"\n",
    "\n",
    "# List and sort .npy files in each folder\n",
    "lr_files = sorted([os.path.join(lr_folder, f) for f in os.listdir(lr_folder) if f.endswith('.npy')])\n",
    "hr_files = sorted([os.path.join(hr_folder, f) for f in os.listdir(hr_folder) if f.endswith('.npy')])\n",
    "\n",
    "assert len(lr_files) == len(hr_files), \"Mismatch: Number of LR and HR files do not match.\"\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_lr, val_lr, train_hr, val_hr = train_test_split(lr_files, hr_files, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = StrongLensingDataset(train_lr, train_hr)\n",
    "val_dataset = StrongLensingDataset(val_lr, val_hr)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e73b422-62c9-4d6a-979b-ac764c1ffc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(SRCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, num_channels, kernel_size=5, padding=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d3bd703-5546-4f7d-a489-931578c94329",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SRCNN(num_channels=1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7284d86-697e-42fa-8f2f-ef99551ad09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.0007, Val Loss: 0.0001\n",
      "Epoch [2/10], Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [3/10], Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [4/10], Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [5/10], Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [6/10], Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [7/10], Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [8/10], Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [9/10], Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [10/10], Train Loss: 0.0001, Val Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for lr_imgs, hr_imgs in train_loader:\n",
    "        # Move tensors to device\n",
    "        lr_imgs = lr_imgs.to(device)   # [B, 1, 75, 75]\n",
    "        hr_imgs = hr_imgs.to(device)   # [B, 1, 150, 150]\n",
    "        \n",
    "        # Upsample LR images to 150x150 using bicubic interpolation\n",
    "        lr_imgs_upsampled = F.interpolate(lr_imgs, size=(150, 150), mode='bicubic', align_corners=False)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        sr_imgs = model(lr_imgs_upsampled)  # Output shape: [B, 1, 150, 150]\n",
    "        loss = criterion(sr_imgs, hr_imgs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * lr_imgs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in val_loader:\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            hr_imgs = hr_imgs.to(device)\n",
    "            lr_imgs_upsampled = F.interpolate(lr_imgs, size=(150, 150), mode='bicubic', align_corners=False)\n",
    "            sr_imgs = model(lr_imgs_upsampled)\n",
    "            loss = criterion(sr_imgs, hr_imgs)\n",
    "            val_loss += loss.item() * lr_imgs.size(0)\n",
    "    \n",
    "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d2c5a79-b7ca-4bce-8046-d4d160f40346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 0.000076\n",
      "Validation PSNR: 41.2130 dB\n",
      "Validation SSIM: 0.9972\n"
     ]
    }
   ],
   "source": [
    "def psnr(sr, hr):\n",
    "    mse_val = ((sr - hr) ** 2).mean().item()\n",
    "    if mse_val == 0:\n",
    "        return 100\n",
    "    return 10 * log10(1 / mse_val)\n",
    "\n",
    "def simple_ssim(sr, hr, C1=0.01**2, C2=0.03**2):\n",
    "    sr_mean = sr.mean()\n",
    "    hr_mean = hr.mean()\n",
    "    sr_var = sr.var()\n",
    "    hr_var = hr.var()\n",
    "    sr_hr_cov = ((sr - sr_mean) * (hr - hr_mean)).mean()\n",
    "    \n",
    "    ssim_val = (2 * sr_mean * hr_mean + C1) * (2 * sr_hr_cov + C2)\n",
    "    denom = (sr_mean**2 + hr_mean**2 + C1) * (sr_var + hr_var + C2)\n",
    "    return (ssim_val / denom).item()\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "mse_list, psnr_list, ssim_list = [], [], []\n",
    "with torch.no_grad():\n",
    "    for lr_imgs, hr_imgs in val_loader:\n",
    "        lr_imgs = lr_imgs.to(device)\n",
    "        hr_imgs = hr_imgs.to(device)\n",
    "        lr_imgs_upsampled = F.interpolate(lr_imgs, size=(150, 150), mode='bicubic', align_corners=False)\n",
    "        sr_imgs = model(lr_imgs_upsampled)\n",
    "        \n",
    "        batch_mse = criterion(sr_imgs, hr_imgs).item()\n",
    "        batch_psnr = psnr(sr_imgs, hr_imgs)\n",
    "        batch_ssim = simple_ssim(sr_imgs, hr_imgs)\n",
    "        \n",
    "        mse_list.append(batch_mse)\n",
    "        psnr_list.append(batch_psnr)\n",
    "        ssim_list.append(batch_ssim)\n",
    "\n",
    "avg_mse = np.mean(mse_list)\n",
    "avg_psnr = np.mean(psnr_list)\n",
    "avg_ssim = np.mean(ssim_list)\n",
    "print(f\"Validation MSE: {avg_mse:.6f}\")\n",
    "print(f\"Validation PSNR: {avg_psnr:.4f} dB\")\n",
    "print(f\"Validation SSIM: {avg_ssim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d1572-0c79-42ef-967b-f7a373b3fef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
